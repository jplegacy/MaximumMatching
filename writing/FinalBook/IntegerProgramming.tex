%Jacob COMPLETE

%\subsection{Integer Programming} \label{IP}

%Introduction
%NP-Hardness
%How to Solve
%Maximum matching via integer programming
%The Matching Polytope

\subsubsection{Introduction}\label{IPintro}

Integer programming is a discrete variant of linear programming.

\begin{definition}
    An integer program is specified by the same information as a linear program: variables, an objective function and constraints.
    A feasible solution for an integer program is a feasible solution for the corresponding linear program, its \textit{LP relaxation}, such that each variable is assigned an integer.
    An optimal solution is a feasible solution with the maximum value amongst feasible solutions.
    The computational problem \textit{integer programming} is defined as in the linear case.
\end{definition}

\begin{remark}
    In integer programming, is the variables that must be integers, not the coordinates of the linear functions involved.
\end{remark}

\begin{example}
    For $\lambda \in \R$, consider the following integer program with variables in two variables $x$ and $y$:

\begin{center}
Maximize $y$

Subject to the constraints

$0 \leq \lambda y - x \leq 0$

$y \geq 1$.

\end{center}

A feasible solution to this integer program is a witness that $\lambda$ is rational.
Hence this linear program is infeasible when $\lambda$ is irrational, while it is unbounded if $\lambda$ is rational.
Note that its LP relaxation is always irrational.
\end{example}

\begin{notation}
    If $I$ is an integer program, we write $\tilde{I}$ for its LP relaxation.
\end{notation}

\begin{example}\label{0-1}
Another variant of integer programming is 0-1 integer programming where integers are replaced with binary values.
It is a special case of integer programming obtained by adding the constraints $0 \leq x \leq 1$ for all variables $x$ to an integer program.
This is useful because binary variable assignments correspond to propositional functions and hence to subsets of the variable set.
\end{example}

\subsubsection{NP-Hardness}\label{IPisNPhard}

We saw earlier that linear programming is in the class P.
Despite its similarity to linear programming, integer programming is much harder.
In fact, we have the following:

\begin{theorem}\label{nphard}\cite{karp21}
    Integer programming is NP-hard.
\end{theorem}

\begin{proof}
In fact, we will show 0-1 integer programming as in \Cref{0-1} is NP hard by reducing SAT to it.
Let $C_{1}, \ldots, C_{m}$ be the clauses of a formula in CNF with variables $x_{1}, \ldots x_{n}$. 
Our 0-1 program will have variables $y_{1}, \ldots, y_{n}$ with objective
    
\begin{center}
Maximize $0$

Subject to the constraints

For all $0 \leq j \leq m$, $\sum_{x_{i} \in C_{j}} (-y_{i}) + \sum_{\neg x_{i} \in C_{j}} y_{i} \leq (\#\text{ of negated variables in } C_{j}) - 1$.

\end{center}

The objective function being constant ensures any feasible solution is optimal, so it suffices to show feasible solutions are the same as satisfying assignments of $\bigwedge_{j = 1}^{m}C_{j}$.
For $y_{1}, \ldots, y_{n} \in \{0, 1\}$ and $0 \leq j \leq m$, observe that $\sum_{x_{i} \in C_{j}} (-y_{i}) + \sum_{\neg x_{i} \in C_{j}} y_{i} \leq (\#\text{ of negated variables in } C_{j}) - 1$ if and only if either a $y_{i}$ in the first sum is 1 or a $y_{i}$ in the second is 0.
Equivalently, clause $C_{j}$ is satisfied under the corresponding variable assignment. The result follows.
\end{proof}

\subsubsection{How to Solve}\label{solvingIP}

In spite of \Cref{nphard}, there are known methods to make integer programming tractable in practice.
The basic idea is as follows.

Suppose given an integer program $I$.
A first approximation to an optimal solution to $I$ would be an optimal solution to its LP relaxation $\tilde{I}$.
Of course, such a solution may not exist. If this is the case, the fundamental theorem of linear programming tells us that either $\tilde{I}$ is infeasible or unbounded.
In the first case, $I$ has no optimal solution and we are done.
The latter is more troublesome, but we ignore this case since it is uncommon in applications.

Say $x$ is an optimal solution to $\tilde{I}$. If $x$ is integral, we are done.
Otherwise, there is a fractional coordinate $\floor{x_i} < x_i < \ceil {x_i}$. Any optimal solution $y$ to $I$ must have $y_{i} \leq \floor{x_i}$ or $\ceil{x_i} \leq y_i$.
Hence our problem breaks into two subproblems $I'$ and $I''$ where $I'$ is $I$ with the added constraint $y_{i} \leq \floor{x_i}$ and $I''$ is $I$ with the added constraint $\ceil{x_i} \leq y_i$.
If we can solve $I'$ and $I''$, the solutions can be pieced together as follows:

\begin{itemize}
    \item If both have optimal solutions, the one with greater objective value works (or either if they have equal objective value).
    \item If only one has an optimal solution, that solution works.
    \item If neither has an optimal solution, $I$ does not either.
\end{itemize}

Thus, we have a recursive algorithm for solving integer programming.
There are two obvious questions:

(1) Does this process terminate?
If the feasible region of $\tilde{I}$ is bounded, the answer is yes.
Indeed, if the feasible region is contained in the rectangle $[a_1, b_1] \times [a_2, b_2] \times \ldots \times [a_n, b_n]$ with $a_{i}, b_{i} \in \Z$, the feasible regions of $\tilde{I'}$ and $\tilde{I''}$ are contained in properly smaller rectangles with integer corners.

(2) Is this algorithm efficient or at least is it usually efficient?
The answer is no, in its current form.
As it is, a lot of the work the algorithm does is superfluous.
For example, suppose we have found an optimal solution $x$ to $I'$ and $x$ has objective value larger than that of the optimal solution to $\tilde{I''}$.
Then also any objective solution to $I''$ also has smaller objective value than $x$, so we can stop immediately and return $x$.
Taking this shortcut cuts the running time in half. 
By taking similarly shortcuts recursively, we can cut the time down more.

In the general context, it is useful to picture the integer programs being searched as a binary tree.
At any time, we store as a bound the largest objective value of integral solutions obtained.
For a leaf associated to an integer program $J$, we branch into two subtrees only if the solution to $\tilde{J}$ has objective value at the least the bound.
This is called the \textit{branch and bound} method and fulfills our goal of solving integer programming quickly.

In practice, integer program solvers use more sophisticated techniques for the branching step, but the general strategy is the same.

\subsubsection{Maximum Matching via Integer Programming}\label{MMviaIP}

\begin{definition}\label{I(G)}
    
Given a hypergraph $G = (V, E)$ and $v \in V$, let $\delta(v)$ denote $\{e \in E \mid v \text{ is incident to } e\}$.
We define the integer program $I(G)$ with variables indexed by $E$ as follows:
\begin{center}
Maximize $\sum_{e \in E} x_e$

Subject to the constraints

(1) For each $e \in E$, $0 \leq x_e$

(2) For each $v \in V$, $\sum_{e \in \delta(v)} x_e \leq 1$.

\end{center}
\end{definition}

\begin{lemma}\label{integerlemma}
    Feasible points for $I(G)$ correspond to matchings of $G$.
\end{lemma}

\begin{proof}
    Let $x \in \R^E$. Given \Cref{I(G)} (1), \Cref{I(G)} (2) is equivalent to ($2^{\prime}$) $x_e \leq 1$ for all $e \in E$ and ($2^{\prime \prime}$) for all $v \in V$ there is at most one $e \in \delta(v)$ such that $x_e = 1$. 
    Integrality along with \Cref{I(G)} (1) and ($2^{\prime}$) is equivalent to all the $x_e$ being binary i.e. $x \in \{0,1\}^E$.
    And $(2^{\prime \prime})$ is equivalent to $\{e \in E \mid x_e = 1\}$ being a matching.
    Thus the bijection $\{0,1\}^E \cong \mathcal{P}(E)$ restricts to a bijection between $x \in \R^E$ satisfying \Cref{I(G)} (1) and (2) to matchings of $G$.
    \end{proof}

\begin{corollary}
    Solving $I(G)$ is tantamount to finding a maximum of $G$.
\end{corollary}

\begin{proof}
    For $x \in \R^E$ satisfying \Cref{I(G)} (1) and (2), $\sum_{e \in E} x_e$ is the size of the corresponding matching as in \Cref{integerlemma}.
    So optimal solutions to $I(G)$ correspond to maximum matchings of $G$.
\end{proof}

Hence the maximum matching problem may be solved by using an integer programming algorithm on the problem $I(G)$.
As integer programming is NP-complete, one cannot expect this solution to run in polynomial time.
In fact, it is not even clear this method will find run in polynomial time when $G$ is bipartite.
To remedy this, we introduce the concept of the matching polytope.

\subsubsection{The Matching Polytope}\label{matchingpolytope}

Unlike its integral counterpart, linear programming can be solved in polynomial time, using the simplex algorithm.
This suggests that we consider the LP relaxation $L(G) = \widetilde{I(G)}$.

\begin{definition}
    The (fractional) \textit{matching polytope} of $G$, denoted $FM(G)$ is the feasible region of $L(G)$ i.e. $\{x \in \R^E \mid \text{ for all }e \in E,\text{ }0 \leq x_e \text{ and for all $v \in V,\text{ }\sum_{e \in \delta(v)} x_e \leq 1$}\}$.
    Let $IM(G)$ be the integer points of $FM(G)$ and $M(G)$ the convex hull of $IM(G)$.
\end{definition}

\Cref{integerlemma} says points of $IM(G)$ correspond to matchings of $G.$
Since $FM(G)$ is convex, $M(G) \subset FM(G)$.

\begin{theorem} \label{matching polytope} \cite{edmonds}
    For $G$ an ordinary graph, $M(G) = FM(G)$ if and only if $G$ is bipartite.
\end{theorem}

\begin{proof}

($\Rightarrow$) Suppose $M(G) = FM(G)$. We will show $G$ contains no odd cycles.
For suppose $C$ is a cycle of odd length $n$; it can be assumed $C$ is simple.
Define $y \in \R^E$ by $y_e = \frac{1}{2}$ for $e \in C$ and $y_e = 0$ otherwise. Since $C$ is simple, $y \in FM(E)$ and so $y \in M(G)$ by assumption. 
    
Now consider $\{x \in \R^E \mid \sum_{e \in C}x_e \leq \frac{n - 1}{2}\}$.
This set of convex and it is not hard to see that it contains $IM(G)$.
Thus it contains $M(G)$ and, in particular, $y$.
This contradicts $\sum_{e \in C}y_e = \frac{n}{2} > \frac{n - 1}{2}$.

($\Leftarrow$) It will suffice to prove every extreme point of $FM(G)$ is in $IM(G)$.
To do this we take $x \in FM(G) - IM(G)$ and show it is not an extreme point.
Let $e_0 \in E$ be such that $0 < x_{e_0} < 1$ and $P$ a maximal simple path or cycle extending $e_0$ such $0 < x_e < 1$ for all $e \in P$.
Set $\epsilon = \min\{x_e, 1 - x_e \mid e \in P\}$, $x'$ to be $x$ with $\epsilon$ added at even edges of $P$ and subtracted at odd edges of $P$ and $x''$ to be $x$ with $\epsilon$ subtracted at even edges of $P$ and added at odd edges of $P$.
These are well defined because $P$ is simple and has even length if it is a cycle.

Since $\epsilon \leq x_{e}$ for all $e \in P$, \Cref{I(G)} (1) holds for $x'$ and $x''$.
Let $v \in V$; we will show $\sum_{e \in \delta(v)} x'_e,$ $\sum_{e \in \delta(v)} x''_e \leq 1$.
If $v$ is not in $P$ or is in $P$ but not an endpoint, $\sum_{e \in \delta(v)} x'_e = \sum_{e \in \delta(v)} x''_e = \sum_{e \in \delta(v)} x_e \leq 1$ by construction.
If $v$ is an endpoint, the condition $\epsilon \leq 1 - x_e$ for $e$ the one edge adjacent to $v$ ensures the condition.
Since $0 < x_e < 1$ for all $e \in P$, we have $0 < \epsilon$ so $x \neq x', x''$.
But $x = \frac{1}{2} x' + \frac{1}{2} x''$, so $x$ is not an extreme point of $FM(G)$.

\end{proof}

\begin{scholium}
    If $G$ is Bipartite, $I(G)$ can be solved in polynomial time with respect to $G$.
\end{scholium}

\begin{proof}
    The proof of \Cref{matching polytope} showed every extreme point of $FM(G)$ is in $I(G)$.
    As the simplex algorithm returns an extreme point of the feasible region, its solution is automatically integral. 
\end{proof}
