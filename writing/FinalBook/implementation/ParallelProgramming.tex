% Ethan O'Farrell, COMPLETE, Reviewer Jack Rubin, DUE DATE: 11/19

\textbf{Threading \& Parallel Programming}

\paragraph{}
\textbf{Parallel programming}, parallel processing, and parallel computing all refer to the concept 

of using computational resources to run different parts of a program in parallel. 

\textbf{Threading}Threading is a form of parallel programming which uses a shared memory space 

within a larger program. 


\subsubsection*{What is the purpose of threading?}


\paragraph{}
Threading serves as a practical way to improve algorithm runtime. Given an algorithm that 

runs in $O(n)$. The best possible improvement we can make to the runtime with threading is 

$O(s+(n/t)$ where $s$ is the fixed time to initialize and end the threads, and $t$ is the 

number of threads used. Of course, the runtime class cannot actually be improved, since 

$O(s+(n/t)$ is still absorbed by $O(n)$. However, this improvement can still greatly benefit 

the practical runtime. For example, given an algorithm that takes $30$ minutes for some $n$ 

size input to run. Altering the program to utilize a system that can handle 8 threads can 

reduce the runtime to under just $4$ minutes. 


\subsubsection*{How does threading relate to Maximum Matching?}


\paragraph{}
Parallel programming strategies generally rely on $1$ key concept: Memory Management. Whether 

or not the parallel parts of the program share a memory space will determine the best 

implementation. Threading benefits from a shared memory space, whereas processes split the 

program into disjoint memory spaces. So when do we use threading versus processes? Generally 

the following rules should be considered when deciding to use threading:


\begin{enumerate}

    \item Can the program be split into smaller programs?
    
    \item Does the order in which these programs run matter?
    
    \item Do the parallel parts of the program share information?

\end{enumerate}


\paragraph{}
Naturally, if the program can only be run sequentially, any form of parallel programming will be 

futile. In order to use threading, the program must be able to run multiple functions at the same 

time. If the order in which the results of parallel functions return matters, then synchronization

is required. For example, say we write a program that solves mathematical equations using PEMDAS. 

If we were to incorporate different threads for addition, subtraction, multiplication, division, 

etc., the order in which each thread computes its part of the equation matters. 


\paragraph{}
Synchronization is especially important when using shared memory spaces. Maximum Matching algorithms 

involve edges matching between various partitions of vertices. Such graphs are a form of shared 

memory. Therefore, when algorithms such as FordFulkerson and EdmondsKarp read and alter various parts 

of the graph, future searches will depend on these changes. Even a brute force approach involves a 

shared memory space. Usually brute force approaches utilize the construction of all subsets of edges

in the given graph. This collection of subsets can also be considered a shared memory space. 


\paragraph{}
The difference between brute force and traditional maximum matching algorithms is how these shared 

memory spaces are used. In the case of graph traversal algorithms, synchronization is necessary for

ensuring that multiple threads do not apply conflicting changes to the same graph at the same time. 

Whereas a brute force algorithm does not entail editing or searching of graphs, but rather adding new 

subsets to a single larger set. 


\subsubsection*{Synchronization}


\paragraph{}
As mentioned before, threading involves the use of a shared memory space. Usually, programmers 

avoid giving multiple sections of a program access to the same variable to avoid side effects. 

However, completely avoiding shared variables in parallel programs would involve a ton of copying. 

Not only is repeatedly cloning such variables a waste of memory space, we also lose most of the runtime

we gain from using threading in the first place. In order to solve this problem, we can employ the 

concept of synchronization to manage the access threads have in the shared memory space. In short,

by "synchronizing" the threads, we can ensure no $2$ threads access and alter the same variables 

at the same time. 


\paragraph{}
A common tool used for synchronization is called a semaphore. In short, we can think of semaphores

like a counting variable. Semaphores track how many threads are accessing or waiting to access

a variable in the shared memory space. When a thread attempts to access a shared variable that

currently being used by other threads, it is placed in a queue based on the value of the semaphore.


\paragraph{}
One must take care when implementing synchronization however. If every variable is locked with a 

semaphore, then threads will never run in parallel. Consequently, every time a thread tries to enter 

the shared memory space, another thread will block it. If the programmer does not manage these trade 

offs the threads only serve to add more overhead to the program.
